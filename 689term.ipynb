{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26dcdc24-c075-415d-a09c-df92dc0e118f",
   "metadata": {},
   "source": [
    "## Airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f04c2-50bb-44ab-8505-15d1172ca435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory\n",
    "directory_path = 'term_project_airbnb/'\n",
    "\n",
    "# Get all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate through each CSV file\n",
    "for file in csv_files:\n",
    "    # Build the full path of the file\n",
    "    file_path = os.path.join(directory_path, file)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract City, State, and Country from the file name\n",
    "    city, state, country = file.replace('.csv', '').split(',')[:3]\n",
    "\n",
    "    # Add attributes to the DataFrame\n",
    "    df['city'] = city.strip()\n",
    "    df['state'] = state.strip()\n",
    "    df['country'] = country.strip()\n",
    "\n",
    "    # Append the current DataFrame to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Reorder columns, placing City, State, Country at the end\n",
    "selected_columns = ['id', 'room_type', 'price',\n",
    "                    'host_id', 'host_name', 'host_total_listings_count',\n",
    "                    'calendar_last_scraped', 'review_scores_rating',\n",
    "                    'city', 'state', 'country']\n",
    "\n",
    "combined_df = combined_df[selected_columns]\n",
    "\n",
    "# Display the result\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946c565-a017-4bfd-b51f-590400e0f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 'price' column to a numeric data type\n",
    "combined_df['price'] = combined_df['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e5772-ecc1-49fa-b6a4-21328006338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# sort date for scd mainenance\n",
    "combined_df = combined_df.sort_values(by=['id', 'calendar_last_scraped'])\n",
    "print(combined_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d93772-620a-415e-abb0-8e588d5c7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the \"calendar_last_scraped\" column to datetime type\n",
    "combined_df['calendar_last_scraped'] = pd.to_datetime(combined_df['calendar_last_scraped'])\n",
    "\n",
    "# Extract year, month, and day information\n",
    "combined_df['year'] = combined_df['calendar_last_scraped'].dt.year\n",
    "combined_df['quarter'] = combined_df['calendar_last_scraped'].dt.quarter\n",
    "combined_df['month'] = combined_df['calendar_last_scraped'].dt.month\n",
    "combined_df['day'] = combined_df['calendar_last_scraped'].dt.day\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(combined_df.head(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ad099-2eb9-431f-8f41-0eef3e19bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique calendar_last_scraped:\")\n",
    "print(combined_df[['year', 'month', 'day']].drop_duplicates())\n",
    "\n",
    "print(\"\\nUnique City State Country:\")\n",
    "print(combined_df[['city', 'state', 'country']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a6318-4c19-4c48-8bb8-65943ebafe22",
   "metadata": {},
   "source": [
    "## LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916fb42-85ee-46ee-ac7e-aa11d4e5baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "job_postings = pd.read_csv('job_postings.csv')\n",
    "company_details = pd.read_csv('companies.csv')\n",
    "\n",
    "merged_data = pd.merge(job_postings, company_details, on='company_id', how='left')\n",
    "merged_data.to_csv('job_companies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fff66-00c9-4112-86a6-c0829e7aa89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_to_read = [\n",
    "    'job_id',\n",
    "    'max_salary',\n",
    "    'med_salary',\n",
    "    'min_salary',\n",
    "    'expiry',\n",
    "    'formatted_experience_level',\n",
    "    'listed_time',\n",
    "    'sponsored',\n",
    "    'work_type',\n",
    "    'name',\n",
    "    'company_size',\n",
    "    'state',\n",
    "    'country',\n",
    "    'city'\n",
    "]\n",
    "\n",
    "jobs_df = pd.read_csv('job_companies.csv', usecols=columns_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0750d-aa38-476a-9a30-a6c695105b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in jobs_df:\", len(jobs_df))\n",
    "jobs_df = jobs_df[(jobs_df['country'].notna()) & (jobs_df['country'] != '0') & (jobs_df['state'] != '0')]\n",
    "print(\"Number of rows in jobs_df:\", len(jobs_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3c483-7189-444d-a655-30e0448dfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique countries:\", jobs_df['country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6d955-eeb9-4e5c-86b8-7357b98e0268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Mapping for state abbreviations\n",
    "state_mapping = {\n",
    "    'CALIFORNIA': 'CA',\n",
    "    'NEWYORK': 'NY',\n",
    "    'TEXAS': 'TX',\n",
    "    'MASSACHUSETTS': 'MA',\n",
    "    'MONTANA': 'MT',\n",
    "    'FLORIDA': 'FL',\n",
    "    'ILLINOIS': 'IL',\n",
    "    'NEVADA': 'NV',\n",
    "    'OHIO': 'OH',\n",
    "    'COLORADO': 'CO',\n",
    "    'HAWAII': 'HI',\n",
    "    'NEWJERSEY': 'NJ',\n",
    "    'TENNESSEE': 'TN',\n",
    "    'LOUISIANA': 'LA',\n",
    "    'OREGON': 'OR',\n",
    "    'RHODEISLAND': 'RI',\n",
    "    'WASHINGTON': 'WA',\n",
    "    'MINNESOTA': 'MN',\n",
    "    'DISTRICTOFCOLUMBIA': 'DC',\n",
    "}\n",
    "\n",
    "# Assume you have already created the jobs_df DataFrame\n",
    "# If not, you can use the previously mentioned code to create it\n",
    "\n",
    "# Create a new DataFrame to store the processed state values\n",
    "processed_jobs_df = jobs_df.copy()\n",
    "\n",
    "# Preprocess state names: remove spaces, special characters, and convert to uppercase\n",
    "processed_jobs_df['state'] = processed_jobs_df['state'].apply(lambda x: re.sub(r'\\W+', '', str(x).upper()))\n",
    "\n",
    "# Apply the state mapping with a default value for unknown states\n",
    "processed_jobs_df['state'] = processed_jobs_df['state'].map(lambda x: state_mapping.get(x, x))\n",
    "\n",
    "# Filter DataFrame to include only rows with valid state mappings\n",
    "valid_states_df = processed_jobs_df[processed_jobs_df['state'].isin(state_mapping.values())]\n",
    "\n",
    "# Count the occurrences of each state and print in the specified order\n",
    "state_counts = valid_states_df[valid_states_df['country'] == 'US']['state'].value_counts()\n",
    "\n",
    "# Print counts in the specified order\n",
    "for state, count in state_counts.items():\n",
    "    print(f\"{state}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a0a8b-e8a5-4130-9f41-770d2ac0dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_states_df['salary'] = valid_states_df.apply(lambda row: row['med_salary'] if not pd.isnull(row['med_salary'])\n",
    "                                                  else (None if pd.isnull(row['min_salary']) and pd.isnull(row['max_salary'])\n",
    "                                                        else (row['min_salary'] + row['max_salary']) / 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083fce7a-195b-45de-819d-c6066672db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'listed_time' and 'expiry' to datetime and extract components\n",
    "valid_states_df['listed_time'] = pd.to_datetime(valid_states_df['listed_time'], unit='ms')\n",
    "valid_states_df['expiry'] = pd.to_datetime(valid_states_df['expiry'], unit='ms')\n",
    "\n",
    "# Extract components for 'listed_time'\n",
    "valid_states_df['list_year'] = valid_states_df['listed_time'].dt.year\n",
    "valid_states_df['list_month'] = valid_states_df['listed_time'].dt.month\n",
    "valid_states_df['list_day'] = valid_states_df['listed_time'].dt.day\n",
    "\n",
    "# Extract components for 'expiry'\n",
    "valid_states_df['expiry_year'] = valid_states_df['expiry'].dt.year\n",
    "valid_states_df['expiry_month'] = valid_states_df['expiry'].dt.month\n",
    "valid_states_df['expiry_day'] = valid_states_df['expiry'].dt.day\n",
    "\n",
    "print(valid_states_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76b739-4c5f-4dc9-9da5-aeb493a86161",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc128db-ac4a-4262-b0cd-4b72c06fec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_copy = combined_df.copy()\n",
    "valid_states_df_copy = valid_states_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68070889-6de0-4374-9589-bc6b268de62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df_copy.copy()\n",
    "valid_states_df = valid_states_df_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701be3b-a4da-4a5c-8b36-dea8cb1ad2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df)\n",
    "print(valid_states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539936c-2a1b-410a-8fce-8bb94d7f6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.dropna(subset=['host_total_listings_count'], inplace=True)\n",
    "combined_df['host_total_listings_count'] = combined_df['host_total_listings_count'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef9142-9f81-4bdb-a0b8-0116db83e951",
   "metadata": {},
   "source": [
    "### location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a2ab3-4a56-4d99-9403-de9821eaf0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract City, State, Country from combined_df\n",
    "combined_locations = combined_df[['city', 'state', 'country']].copy()\n",
    "combined_locations = combined_locations.drop_duplicates()\n",
    "#print(combined_locations)\n",
    "# Extract state, country, city from valid_states_df\n",
    "valid_states_locations = valid_states_df[['state', 'country', 'city']].copy()\n",
    "valid_states_locations = valid_states_locations.drop_duplicates()\n",
    "#print(valid_states_locations)\n",
    "\n",
    "# Combine all locations and create a unique ID starting from 0\n",
    "all_locations = pd.concat([combined_locations, valid_states_locations], ignore_index=True)\n",
    "all_locations = all_locations.drop_duplicates()\n",
    "\n",
    "# Add a unique ID column starting from 0\n",
    "#all_locations['location_id'] = range(len(all_locations))\n",
    "all_locations['location_id'] = range(1, len(all_locations) + 1)\n",
    "\n",
    "# Print the resulting DataFrame with unique ID starting from 0\n",
    "print(all_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76141799-b4e1-4656-8d75-07197ba3622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge location_id with combined_df\n",
    "combined_df = pd.merge(combined_df, all_locations, how='left', on=['city', 'state', 'country'])\n",
    "\n",
    "# Merge location_id with valid_states_df\n",
    "valid_states_df = pd.merge(valid_states_df, all_locations, how='left', left_on=['city', 'state', 'country'], right_on=['city', 'state', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bed3d5-9e71-4526-9a02-94641d24a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 'city', 'state', 'country' columns in combined_df\n",
    "print(combined_df[['city', 'state', 'country', 'location_id']])\n",
    "\n",
    "# Print 'city', 'state', 'country' columns in valid_states_df\n",
    "print(valid_states_df[['city', 'state', 'country', 'location_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773403b-d239-453a-89bc-cfdb7f5fa103",
   "metadata": {},
   "source": [
    "#### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3bb86e-afb4-4ee0-82ee-1f65f5544032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "conn_str = 'DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term;Trusted_Connection=yes;'\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for index, row in all_locations.iterrows():\n",
    "    cursor.execute(\"INSERT INTO Location (id, country, state, city) VALUES (?, ?, ?, ?)\",\n",
    "                   row['location_id'], row['country'], row['state'], row['city'])\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7dd7db-a5d9-4b7a-a386-a24800b7f62d",
   "metadata": {},
   "source": [
    "### time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b596fd-eb5b-46ca-81d2-ce406b0c2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df[['year', 'quarter', 'month', 'day']])\n",
    "print(valid_states_df[['list_year', 'list_month', 'list_day', 'expiry_year', 'expiry_month', 'expiry_day']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b6421-fd19-4527-af71-9933fc6a7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'year', 'month', and 'day' columns from both DataFrames\n",
    "combined_dates = combined_df[['year', 'month', 'day']]\n",
    "valid_states_list = valid_states_df[['list_year', 'list_month', 'list_day']].rename(columns={'list_year': 'year', 'list_month': 'month', 'list_day': 'day'})\n",
    "valid_states_expiry = valid_states_df[['expiry_year', 'expiry_month', 'expiry_day']].rename(columns={'expiry_year': 'year', 'expiry_month': 'month', 'expiry_day': 'day'})\n",
    "\n",
    "# Concatenate the DataFrames vertically\n",
    "all_dates = pd.concat([combined_dates, valid_states_list, valid_states_expiry])\n",
    "\n",
    "# Remove duplicate rows based on 'year', 'month', and 'day'\n",
    "all_dates_no_duplicates = all_dates.drop_duplicates(subset=['year', 'month', 'day'])\n",
    "\n",
    "# Sort the DataFrame by 'year', 'month', and 'day'\n",
    "all_dates_sorted = all_dates_no_duplicates.sort_values(by=['year', 'month', 'day'])\n",
    "\n",
    "# Add a new column 'date_id' with unique values starting from 1\n",
    "all_dates_sorted['date_id'] = range(1, len(all_dates_sorted) + 1)\n",
    "\n",
    "print(all_dates_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f5956-cd14-447a-8d74-8a4bfe75fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "conn_str = 'DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term;Trusted_Connection=yes;'\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "for index, row in all_dates_sorted.iterrows():\n",
    "    cursor.execute(\"INSERT INTO DayTime (id, year, month, day) VALUES (?, ?, ?, ?)\",\n",
    "               int(row['date_id']), int(row['year']), int(row['month']), int(row['day']))\n",
    "\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca430fc0-e5b2-4548-9448-5b000b7198bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge date_id with combined_df\n",
    "combined_df = pd.merge(combined_df, all_dates_sorted, how='left', on=['year', 'month', 'day'])\n",
    "\n",
    "# Merge date_id with valid_states_df for list_date_id\n",
    "valid_states_df = pd.merge(valid_states_df, all_dates_sorted, how='left', left_on=['list_year', 'list_month', 'list_day'], right_on=['year', 'month', 'day'])\n",
    "valid_states_df = valid_states_df.rename(columns={'date_id': 'list_date_id'})\n",
    "\n",
    "# Merge date_id with valid_states_df for expiry_date_id\n",
    "valid_states_df = pd.merge(valid_states_df, all_dates_sorted, how='left', left_on=['expiry_year', 'expiry_month', 'expiry_day'], right_on=['year', 'month', 'day'])\n",
    "valid_states_df = valid_states_df.rename(columns={'date_id': 'expiry_date_id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ffca1-e068-4a63-b27e-4d3126dd87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df[['year', 'month', 'day', 'date_id']])\n",
    "print(valid_states_df[['list_year', 'list_month', 'list_day', 'list_date_id']])\n",
    "print(valid_states_df[['expiry_year', 'expiry_month', 'expiry_day', 'expiry_date_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822ff79-ca2a-42f6-b371-e6fcdf663f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df[['year', 'quarter', 'month']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5867037-52a8-47ec-b940-c35482a456fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'list_year', 'list_quarter', 'list_month' columns from valid_states_df\n",
    "list_months = combined_df[['year', 'quarter', 'month']]\n",
    "\n",
    "# Remove duplicate rows based on 'list_year', 'list_quarter', and 'list_month'\n",
    "unique_list_months = list_months.drop_duplicates()\n",
    "\n",
    "# Sort the DataFrame by 'list_year', 'list_quarter', and 'list_month'\n",
    "sorted_list_months = unique_list_months.sort_values(by=['year', 'quarter', 'month'])\n",
    "\n",
    "# Add a new column 'list_month_id' with unique values starting from 1\n",
    "sorted_list_months['month_id'] = range(1, len(sorted_list_months) + 1)\n",
    "\n",
    "# Merge 'list_month_id' with valid_states_df\n",
    "combined_df = pd.merge(combined_df, sorted_list_months, how='left', on=['year', 'quarter', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77defc1a-5481-4480-a579-0e280a6afa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_list_months)\n",
    "#print(valid_states_df['list_month_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90541d81-c8f7-4498-8071-a3caab0acb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "conn_str = 'DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term;Trusted_Connection=yes;'\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "for index, row in sorted_list_months.iterrows():\n",
    "    cursor.execute(\"INSERT INTO MonthTime (id, year, quarter, month) VALUES (?, ?, ?, ?)\",\n",
    "               int(row['month_id']), int(row['year']), int(row['quarter']), int(row['month']))\n",
    "\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3da42-80c1-43bc-ba79-07a2d0a6bb67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### company SCD 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40343b-7fa4-41aa-84b8-fc6aa54bd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = valid_states_df[['name', 'company_size']]\n",
    "unique_companies = companies.drop_duplicates()\n",
    "sorted_companies = unique_companies.sort_values(by=['name', 'company_size'])\n",
    "\n",
    "if 'company_id' in valid_states_df.columns:\n",
    "    valid_states_df = valid_states_df.drop(columns=['company_id'])\n",
    "\n",
    "sorted_companies['company_id'] = range(1, len(sorted_companies) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139b965-228b-4ae9-870d-92ffecd5308d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sorted_companies)\n",
    "print(valid_states_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14858d01-f6c0-481a-8146-52b2d590f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_states_df = pd.merge(valid_states_df, sorted_companies, how='left', on=['name', 'company_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901cb48-41b6-4330-a98d-222fac4d13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "conn_str = 'DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term;Trusted_Connection=yes;'\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for index, row in sorted_companies.iterrows():\n",
    "\n",
    "    company_id = int(row['company_id']) if not pd.isna(row['company_id']) else None\n",
    "    company_size = int(row['company_size']) if not pd.isna(row['company_size']) else None\n",
    "    \n",
    "    cursor.execute(\"INSERT INTO Company (id, name, company_size) VALUES (?, ?, ?)\",\n",
    "                   company_id, row['name'], company_size)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecafe16e-cbdc-4910-968a-a324f1bd7622",
   "metadata": {},
   "source": [
    "### job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bdd61-7693-44b7-9539-484a235e49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'work_type', 'formatted_experience_level', 'sponsored' columns from valid_states_df\n",
    "job_description_columns = valid_states_df[['work_type', 'formatted_experience_level', 'sponsored']]\n",
    "\n",
    "# Remove duplicate rows based on the selected columns\n",
    "unique_job_descriptions = job_description_columns.drop_duplicates()\n",
    "\n",
    "# Add a new column 'job_description_id' with unique values starting from 1\n",
    "unique_job_descriptions['job_description_id'] = range(1, len(unique_job_descriptions) + 1)\n",
    "\n",
    "# Merge 'job_description_id' with valid_states_df\n",
    "valid_states_df = pd.merge(valid_states_df, unique_job_descriptions, how='left', on=['work_type', 'formatted_experience_level', 'sponsored'])\n",
    "print(unique_job_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7e284-85c7-41c7-8cf9-6ebd7b5f2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "conn_str = 'DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term;Trusted_Connection=yes;'\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for index, row in unique_job_descriptions.iterrows():\n",
    "    \n",
    "    formatted_experience_level = str(row['formatted_experience_level']) if pd.notna(row['formatted_experience_level']) else \"\"\n",
    "    \n",
    "    cursor.execute(\"INSERT INTO Job_description (id, work_type, experience_level_required, sponsored) VALUES (?, ?, ?, ?)\",\n",
    "                   int(row['job_description_id']), row['work_type'], formatted_experience_level, int(row['sponsored']))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0e229-a5ec-4bc6-8eb6-3e7eaffeca6e",
   "metadata": {},
   "source": [
    "### room SCD 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3411d-0c2c-4d16-a773-175bc8a65b59",
   "metadata": {},
   "source": [
    "#### for test purpose only!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b0f525-591f-485d-9749-83588c4f4821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   room_id           airbnb_key     room_type  price effect_date expire_date  \\\n",
      "0        1  1016619106223207155  Entire place  349.0  2023-09-18  9999-12-31   \n",
      "1        1  1016619106223207155  Entire place  300.0  2023-08-18  2023-09-17   \n",
      "\n",
      "  current_flag  \n",
      "0            Y  \n",
      "1            N  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALIENWARE\\AppData\\Local\\Temp\\ipykernel_33612\\4012980232.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  staging_room_df = pd.read_sql_query(sql_query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Establish connection\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term_test;Trusted_Connection=yes;')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL query to fetch data\n",
    "sql_query = \"\"\"\n",
    "SELECT\n",
    "    [room_id],\n",
    "    [airbnb_key],\n",
    "    [room_type],\n",
    "    [price],\n",
    "    [effect_date],\n",
    "    [expire_date],\n",
    "    [current_flag]\n",
    "FROM\n",
    "    [term_test].[dbo].[Room]\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and load data into Pandas DataFrame\n",
    "staging_room_df = pd.read_sql_query(sql_query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(staging_room_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a061f1fd-1912-4eb4-8085-d8460f958cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id calendar_last_scraped     room_type  price\n",
      "0                    1            2023-01-01  Entire place  100.0\n",
      "1                    2            2023-01-02  Private room   75.0\n",
      "2                    1            2023-01-05  Entire place  110.0\n",
      "3                    2            2023-01-07  Private room   75.0\n",
      "4                    1            2023-01-10   Shared room   50.0\n",
      "5  1016619106223207155            2023-01-10   Shared room  350.0\n",
      "id                        object\n",
      "calendar_last_scraped     object\n",
      "room_type                 object\n",
      "price                    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Test delta data\n",
    "test_df_data = {\n",
    "    'id': ['1', '2', '1', '2', '1', '1016619106223207155'],\n",
    "    'calendar_last_scraped': ['2023-01-01', '2023-01-02', '2023-01-05', '2023-01-07', '2023-01-10', '2023-01-10'],\n",
    "    'room_type': ['Entire place', 'Private room', 'Entire place', 'Private room', 'Shared room', 'Shared room'],\n",
    "    'price': [100.00, 75.00, 110.00, 75.00, 50.00, 350.00]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "test_df = pd.DataFrame(test_df_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(test_df)\n",
    "\n",
    "# Display the data types of each column\n",
    "print(test_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eebee1c-879e-46e6-b483-5faa08a89583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1016619106223207155': 1}\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary as a map\n",
    "room_map = {}\n",
    "\n",
    "# Iterate through each row in staging_room_df and map airbnb_key to room_id\n",
    "for index, row in staging_room_df.iterrows():\n",
    "    airbnb_key = row['airbnb_key']\n",
    "    room_id = row['room_id']\n",
    "    room_map[airbnb_key] = room_id\n",
    "\n",
    "# Print the contents of the dictionary to check if it's correct\n",
    "print(room_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a0801d-2086-4793-b375-80222bb73db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2']\n",
      "{'1016619106223207155': 1, '1': 2, '2': 3}\n"
     ]
    }
   ],
   "source": [
    "#room_map = {}\n",
    "\n",
    "# Use the default parameter to handle an empty dictionary\n",
    "max_value = max(set(room_map.values()), default=None)\n",
    "\n",
    "if max_value is None:\n",
    "    max_value = 0\n",
    "\n",
    "count = max_value + 1\n",
    "\n",
    "distinct_ids = set(test_df['id'].unique())\n",
    "\n",
    "# Subtract the set of room_map keys from distinct_ids\n",
    "result_set = distinct_ids - set(room_map.keys())\n",
    "result_set = sorted(result_set)\n",
    "print(result_set)\n",
    "\n",
    "for key in result_set:\n",
    "    room_map[key] = count\n",
    "    count += 1\n",
    "\n",
    "print(room_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "841631ef-ebaa-46ae-875b-4595eed05dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id calendar_last_scraped     room_type  price  room_id\n",
      "0                    1            2023-01-01  Entire place  100.0        2\n",
      "1                    2            2023-01-02  Private room   75.0        3\n",
      "2                    1            2023-01-05  Entire place  110.0        2\n",
      "3                    2            2023-01-07  Private room   75.0        3\n",
      "4                    1            2023-01-10   Shared room   50.0        2\n",
      "5  1016619106223207155            2023-01-10   Shared room  350.0        1\n"
     ]
    }
   ],
   "source": [
    "test_df['room_id'] = test_df['id'].map(room_map)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ed3743-836c-4932-aeae-d8be5a7268b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming test_df is your DataFrame\n",
    "# You need to replace 'id' with the actual column name in your DataFrame\n",
    "# test_df['room_id'] = test_df['id'].map(room_map)\n",
    "\n",
    "process_df = test_df\n",
    "# process_df = pd.DataFrame(combined_df)\n",
    "\n",
    "# convert the 'calendar_last_scraped' column to date format and \n",
    "# sort the DataFrame based on 'room_id' and in chronological order for (SCD2) maintenance.\n",
    "process_df['calendar_last_scraped'] = pd.to_datetime(process_df['calendar_last_scraped'])\n",
    "process_df_sorted = process_df.sort_values(by=['room_id', 'calendar_last_scraped'])\n",
    "\n",
    "# initialize a dictionary (room_records) to store room records and an index (room_index) for the recent updated index.\n",
    "room_records = {}\n",
    "room_index = {}\n",
    "\n",
    "# Populate room_records with initial records from staging_room_df\n",
    "# Assuming staging_room_df is your DataFrame containing room_id, airbnb_key, room_type, price, etc.\n",
    "for index, row in staging_room_df.iterrows():\n",
    "    room_records[len(room_records) + 1] = {\n",
    "        'room_id': row['room_id'],\n",
    "        'airbnb_key': row['airbnb_key'],\n",
    "        'room_type': row['room_type'],\n",
    "        'price': row['price'],\n",
    "        'effect_date': row['effect_date'],\n",
    "        'expire_date': row['expire_date'],\n",
    "        'current_flag': row['current_flag']\n",
    "    }\n",
    "    room_index[(row['room_id'], row['current_flag'])] = room_records[len(room_records)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608aad70-9281-4d74-a285-e183738cd4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   room_id           airbnb_key     room_type  price effect_date expire_date  \\\n",
      "0        1  1016619106223207155  Entire place  349.0  2023-09-18  2023-01-10   \n",
      "1        1  1016619106223207155  Entire place  300.0  2023-08-18  2023-09-17   \n",
      "2        1  1016619106223207155   Shared room  350.0  2023-01-10  9999-12-31   \n",
      "3        2                    1  Entire place  100.0  2023-01-01  2023-01-05   \n",
      "4        2                    1  Entire place  110.0  2023-01-05  2023-01-10   \n",
      "5        2                    1   Shared room   50.0  2023-01-10  9999-12-31   \n",
      "6        3                    2  Private room   75.0  2023-01-02  9999-12-31   \n",
      "\n",
      "  current_flag  \n",
      "0            N  \n",
      "1            N  \n",
      "2            Y  \n",
      "3            N  \n",
      "4            N  \n",
      "5            Y  \n",
      "6            Y  \n"
     ]
    }
   ],
   "source": [
    "# Iterating through the sorted DataFrame\n",
    "for index, row in process_df_sorted.iterrows():\n",
    "    unique_id = row['room_id']\n",
    "    airbnb_key = row['id']\n",
    "    current_flag = 'Y'\n",
    "\n",
    "    # Check if there is an existing record with the same room_id and current_flag 'Y' using the index\n",
    "    existing_record = room_index.get((unique_id, current_flag))\n",
    "\n",
    "    if existing_record:\n",
    "        # Existing record found, check if relevant columns are the same\n",
    "        if (\n",
    "            row['room_type'] == existing_record['room_type'] and\n",
    "            row['price'] == existing_record['price']\n",
    "        ):\n",
    "            continue\n",
    "        else:\n",
    "            # Relevant columns are different, expire the existing record and insert a new one\n",
    "            expire_date = row['calendar_last_scraped'].strftime(\"%Y-%m-%d\")\n",
    "            existing_record['expire_date'] = expire_date\n",
    "            existing_record['current_flag'] = 'N'\n",
    "\n",
    "    # No existing record found or existing record has current_flag 'N', it's a new record\n",
    "    effect_date = row['calendar_last_scraped'].strftime(\"%Y-%m-%d\")\n",
    "    expire_date = '9999-12-31'\n",
    "\n",
    "    # Create a new record in room_records and update the index\n",
    "    room_records[len(room_records) + 1] = {\n",
    "        'room_id': unique_id,\n",
    "        'airbnb_key': airbnb_key,\n",
    "        'room_type': row['room_type'],\n",
    "        'price': row['price'],\n",
    "        'effect_date': effect_date,\n",
    "        'expire_date': expire_date,\n",
    "        'current_flag': current_flag\n",
    "    }\n",
    "    room_index[(unique_id, current_flag)] = room_records[len(room_records)]\n",
    "\n",
    "# Create a DataFrame from room_records\n",
    "room_df = pd.DataFrame(list(room_records.values()))\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(room_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b51e5444-2faa-4c5d-854b-5dbc6f43e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Create a connection and cursor\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term_test;Trusted_Connection=yes;')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Delete all records from the Room table\n",
    "cursor.execute(\"DELETE FROM Room\")\n",
    "\n",
    "# Commit changes\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the Room table\n",
    "for index, row in room_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Room (room_id, airbnb_key, room_type, price, effect_date, expire_date, current_flag)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", row['room_id'], row['airbnb_key'], row['room_type'], row['price'], row['effect_date'], row['expire_date'], row['current_flag'])\n",
    "\n",
    "# Commit changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d224f8-f3c1-4478-9731-9880cb78578e",
   "metadata": {},
   "source": [
    "#### prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588e797-3827-4ca8-b19e-b5133849f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_id_df = combined_df[['id']].copy()\n",
    "new_id_df = new_id_df.drop_duplicates().reset_index(drop=True)\n",
    "new_id_df['new_id'] = range(1, len(new_id_df) + 1)\n",
    "print(new_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0c130-e4b1-4511-8966-4b7598d16086",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(combined_df, new_id_df, how='left', on=['id'])\n",
    "combined_df = combined_df.drop(columns=['id'])\n",
    "combined_df = combined_df.rename(columns={'new_id': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7627b-ae07-40c1-93ff-073de0239f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(combined_df))\n",
    "print(len(combined_df['id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e483d09-7b3b-4ca5-bb80-f4f4908dec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.rename(columns={'id': 'room_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21bc95-c8ed-4ada-80bc-101fd371d843",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad952f-1b39-4fec-b927-6cb9c75b3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# test delta data\n",
    "test_data = {'room_id': [1, 2, 1, 2, 1, 3],\n",
    "        'calendar_last_scraped': ['2023-01-01', '2023-01-02', '2023-01-05', '2023-01-07', '2023-01-10', '2023-01-10'],\n",
    "        'room_type': ['Entire place', 'Private room', 'Entire place', 'Private room', 'Shared room', 'Shared room'],\n",
    "        'price': [100.00, 75.00, 110.00, 75.00, 50.00, 350.00]}\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd72c7-e6df-4440-8906-6a18fd56f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "process_df = pd.DataFrame(test_df)\n",
    "#process_df = pd.DataFrame(combined_df)\n",
    "\n",
    "# convert the 'calendar_last_scraped' column to date format and \n",
    "#sort the DataFrame based on 'room_id' and in chronological order for (SCD2) maintenance.\n",
    "process_df['calendar_last_scraped'] = pd.to_datetime(process_df['calendar_last_scraped'])\n",
    "process_df_sorted = process_df.sort_values(by=['room_id', 'calendar_last_scraped'])\n",
    "\n",
    "# initialize a dictionary (room_records) to store room records and an index (room_index) for the recent updated index.\n",
    "room_records = {}\n",
    "room_index = {}\n",
    "\n",
    "# Iterating through the sorted DataFramecheck \n",
    "for index, row in process_df_sorted.iterrows():\n",
    "    unique_id = row['room_id']\n",
    "    current_flag = 'Y'\n",
    "\n",
    "    # Check if there is an existing record with the same room_id and current_flag 'Y' using the index\n",
    "    existing_record = room_index.get((unique_id, current_flag))\n",
    "\n",
    "    if existing_record:\n",
    "        # Existing record found, check if relevant columns are the same\n",
    "        if (\n",
    "            row['room_type'] == existing_record['room_type'] and\n",
    "            row['price'] == existing_record['price']\n",
    "        ):\n",
    "            continue\n",
    "        else:\n",
    "            # Relevant columns are different, expire the existing record and insert a new one\n",
    "            expire_date = row['calendar_last_scraped'].strftime(\"%Y-%m-%d\")\n",
    "            existing_record['expire_date'] = expire_date\n",
    "            existing_record['current_flag'] = 'N'\n",
    "\n",
    "    # No existing record found or existing record has current_flag 'N', it's a new record\n",
    "    effect_date = row['calendar_last_scraped']\n",
    "    expire_date = '9999-12-31'\n",
    "\n",
    "    # Create a new record in room_records and update the index\n",
    "    room_records[len(room_records) + 1] = {\n",
    "        'room_id': unique_id,\n",
    "        'room_type': row['room_type'],\n",
    "        'price': row['price'],\n",
    "        'effect_date': effect_date,\n",
    "        'expire_date': expire_date,\n",
    "        'current_flag': current_flag\n",
    "    }\n",
    "    room_index[(unique_id, current_flag)] = room_records[len(room_records)]\n",
    "\n",
    "# Create a DataFrame from room_records\n",
    "room_df = pd.DataFrame(list(room_records.values()))\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(room_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd2136-9d2f-400f-88db-d69fd4480ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(room_df))\n",
    "print(len(room_df['room_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba997e-3ac9-4ae9-a575-6a318175b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Create a connection and cursor\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term;Trusted_Connection=yes;')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert data into the Room table\n",
    "for index, row in room_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Room (room_id, room_type, price, effect_date, expire_date, current_flag)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", row['room_id'], row['room_type'], row['price'], row['effect_date'], row['expire_date'], row['current_flag'])\n",
    "\n",
    "# Commit changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d49cc1-e268-4430-bb67-2157500a6593",
   "metadata": {},
   "source": [
    "### host SCD 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb27bc2-dd55-444f-a793-7295390a2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for host_id\n",
    "new_host_id_df = combined_df[['host_id']].copy()\n",
    "new_host_id_df = new_host_id_df.drop_duplicates().reset_index(drop=True)\n",
    "new_host_id_df['new_host_id'] = range(1, len(new_host_id_df) + 1)\n",
    "print(new_host_id_df)\n",
    "\n",
    "# Merge the new_host_id_df with combined_df\n",
    "combined_df = pd.merge(combined_df, new_host_id_df, how='left', on=['host_id'])\n",
    "\n",
    "# Drop the original host_id column\n",
    "combined_df = combined_df.drop(columns=['host_id'])\n",
    "\n",
    "# Rename the new_host_id column to host_id\n",
    "combined_df = combined_df.rename(columns={'new_host_id': 'host_id'})\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11993005-6b8d-4b20-923e-255e5f86e09a",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207dd1a-6fb7-4233-8fd1-594b7c7a2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test delta data\n",
    "test_host_data = {'host_id': [1, 2, 1, 2, 1, 3],\n",
    "        'calendar_last_scraped': ['2023-01-01', '2023-01-02', '2023-01-05', '2023-01-07', '2023-01-10', '2023-01-10'],\n",
    "        'host_name': ['A', 'B', 'A', 'B', 'C', 'C'],\n",
    "        'host_total_listings_count': [100, 75, 110, 75, 50, 350]}\n",
    "\n",
    "test_host_df = pd.DataFrame(test_host_data)\n",
    "print(test_host_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378d0f8-1741-42d2-a795-2a62bbe8294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "process_df = pd.DataFrame(test_host_df)\n",
    "#process_df = pd.DataFrame(combined_df)\n",
    "\n",
    "# Convert 'calendar_last_scraped' to datetime\n",
    "# Sort the DataFrame by 'host_id' and 'calendar_last_scraped' for SCD maintenance\n",
    "process_df['calendar_last_scraped'] = pd.to_datetime(process_df['calendar_last_scraped'])\n",
    "process_df_sorted = process_df.sort_values(by=['host_id', 'calendar_last_scraped'])\n",
    "\n",
    "# initialize a (host_records) to store host records and an (host_index) for the recent updated index.\n",
    "host_records = {}\n",
    "host_index = {}\n",
    "\n",
    "# Iterate through each row in process_df_sorted\n",
    "for index, row in process_df_sorted.iterrows():\n",
    "    unique_id = row['host_id']\n",
    "\n",
    "    # Check if there is an existing record with the same host_id using the index\n",
    "    existing_record = host_index.get(unique_id)\n",
    "\n",
    "    if existing_record:\n",
    "        # Existing record found, check if relevant columns are the same\n",
    "        if row['host_name'] != existing_record['host_name']:\n",
    "            # Relevant column 'host_name' has changed, update the existing record with historical values\n",
    "            timestamp = row[\"calendar_last_scraped\"].strftime('%Y-%m')\n",
    "            existing_record[f'host_name_{timestamp}'] = existing_record['host_name']\n",
    "            existing_record['host_name'] = row['host_name']\n",
    "\n",
    "        if row['host_total_listings_count'] != existing_record['host_total_listings_count']:\n",
    "            # Relevant column 'host_total_listings_count' has changed, update the existing record with historical values\n",
    "            timestamp = row[\"calendar_last_scraped\"].strftime('%Y-%m')\n",
    "            existing_record[f'host_total_listings_count_{timestamp}'] = existing_record['host_total_listings_count']\n",
    "            existing_record['host_total_listings_count'] = row['host_total_listings_count']\n",
    "\n",
    "    else:\n",
    "        # No existing record found, it's a new record\n",
    "        host_records[unique_id] = {\n",
    "            'host_id': unique_id,\n",
    "            'host_name': row['host_name'],\n",
    "            'host_total_listings_count': row['host_total_listings_count']\n",
    "        }\n",
    "        host_index[unique_id] = host_records[len(host_records)]\n",
    "\n",
    "# Create a DataFrame from host_records\n",
    "host_df = pd.DataFrame(list(host_records.values()))\n",
    "host_df['host_total_listings_count'] = host_df['host_total_listings_count'].astype('int64')\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(host_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61162de-c68c-4612-94ba-ef853776e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(host_df.columns)\n",
    "print(host_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7c9511-d34d-4548-8d27-daad98aeffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in host_df.columns:\n",
    "    if host_df[column].dtype == 'O':  # 'O' represents object (string) type\n",
    "        host_df[column] = host_df[column].str.replace(',', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93151a03-31de-41ef-a060-673ef199d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_df.to_csv('Host.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20249a6b-15aa-4b3f-bfa8-fe1c2f4cbfe3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c83c1-31a8-4025-8c4f-4cbc2bc5d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming combined_df is your DataFrame\n",
    "# Aggregate data to calculate average_rent and total_counts\n",
    "summary_df = combined_df.groupby(['month_id', 'location_id']).agg(\n",
    "    average_rent=('price', 'mean'),\n",
    "    total_counts=('room_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Create a DataFrame for monthlyRoomReservationSummary\n",
    "monthly_summary_fact = pd.DataFrame({\n",
    "    'month_id': summary_df['month_id'],\n",
    "    'location_id': summary_df['location_id'],\n",
    "    'average_rent': summary_df['average_rent'],\n",
    "    'total_counts': summary_df['total_counts']\n",
    "})\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(monthly_summary_fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12b8f1-f68b-415e-b5f8-8ee9480baa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Create a connection and cursor\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term;Trusted_Connection=yes;')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert data into the Room table\n",
    "for index, row in monthly_summary_fact.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO MonthlyRoomReservationSummary (month_id, location_id, average_rent, total_counts)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    \"\"\", row['month_id'], row['location_id'], row['average_rent'], row['total_counts'])\n",
    "\n",
    "# Commit changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bc905-c6e8-4730-a869-f778a521ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c088331-a57f-44c4-bb0a-490f11ec2851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Create a connection and cursor\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term;Trusted_Connection=yes;')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert data into the Room table\n",
    "for index, row in combined_df.iterrows():\n",
    "    # Replace NaN with None in the 'review_scores_rating' column\n",
    "    row['review_scores_rating'] = row['review_scores_rating'] if pd.notnull(row['review_scores_rating']) else None\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO List (location_id, time_id, room_id, host_id, review_rating)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\", row['location_id'], row['date_id'], row['room_id'], row['host_id'], row['review_scores_rating'])\n",
    "\n",
    "\n",
    "# Commit changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8e6e7-7b26-47f4-b6c1-0440726caa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_states_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0393a9-b684-4bf7-a416-1778d8f4523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_states_df[['location_id', 'job_description_id', 'company_id', 'expiry_date_id', 'list_date_id', 'salary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db672b94-276e-4a9b-8c9f-ebc09fa450b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Create a connection and cursor\n",
    "conn = pyodbc.connect('DRIVER={SQL Server};SERVER=DESKTOP-14JKRFU\\\\MSSQLSERVER01;DATABASE=term;Trusted_Connection=yes;')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert data into the Room table\n",
    "for index, row in valid_states_df.iterrows():\n",
    "    row['salary'] = row['salary'] if pd.notnull(row['salary']) else None\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Job (location_id, job_description_id, company_id, expiry, listed_time, med_salary)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", row['location_id'], row['job_description_id'], row['company_id'], row['expiry_date_id'], row['list_date_id'], row['salary'])\n",
    "\n",
    "\n",
    "# Commit changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca20fea-f876-41c4-82ab-759a791a125e",
   "metadata": {},
   "source": [
    "# alternative code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
